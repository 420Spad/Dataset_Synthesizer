<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.8"/>
<title>NVIDIA DeepLearning Dataset Synthesizer (NDDS): Trainer</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { searchBox.OnSelectItem(0); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">NVIDIA DeepLearning Dataset Synthesizer (NDDS)
   </div>
  </td>
   <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.8 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('trainer.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
<a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(0)"><span class="SelectionMark">&#160;</span>All</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(1)"><span class="SelectionMark">&#160;</span>Classes</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(2)"><span class="SelectionMark">&#160;</span>Namespaces</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(3)"><span class="SelectionMark">&#160;</span>Functions</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(4)"><span class="SelectionMark">&#160;</span>Variables</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(5)"><span class="SelectionMark">&#160;</span>Typedefs</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(6)"><span class="SelectionMark">&#160;</span>Pages</a></div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Trainer </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><hr/>
<p> <a class="anchor" id="contents"></a> </p><h1>Contents:</h1>
<ul>
<li><a href="#summary">Summary</a></li>
<li><a href="#context">Context</a><ul>
<li><a href="#rl_context">The RL Problem</a></li>
<li><a href="#sl_context">The SL Problem</a></li>
</ul>
</li>
<li><a href="#config">Trainer Configuration</a><ul>
<li><a href="#config_overview">Overview</a></li>
<li><a href="#config_semantics">Special JSON-Parsing Semantics</a></li>
<li><a href="#config_defaults">Parameter Defaults</a></li>
<li><a href="#config_trainer">Trainer Configuration File</a></li>
</ul>
</li>
<li><a href="#config_sl_service">Using the SL Service</a><ul>
<li><a href="#config_sl_json">SL JSON Configuration</a></li>
<li><a href="#config_sl_python">SL Network Implementations</a></li>
<li><a href="#config_sl_output">SL Network Output</a></li>
<li><a href="#sl_coordination">Coordinating SL Networks</a></li>
</ul>
</li>
<li><a href="#config_rl_service">Using the RL Service</a><ul>
<li><a href="#config_rl_dltrainer">DLTrainer Configuration Files</a></li>
<li><a href="#config_rl_dlenv">DLEnvironment Configuration Files</a></li>
<li><a href="#rl_impls">Network Implementations</a></li>
<li><a href="#rl_network_examples">Examples</a><ul>
<li><a href="#rl_network_ex1">Same default network, multiple copies</a></li>
<li><a href="#rl_network_ex2">Multiple default networks, single copy of each</a></li>
<li><a href="#rl_network_ex3">Multiple default networks, multiple copies</a></li>
<li><a href="#rl_network_ex4">Multiple AI networks</a></li>
<li><a href="#rl_network_ex5">Multiple DLEnv sim types</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<hr/>
<p> <a class="anchor" id="summary"></a> </p><h1>Summary</h1>
<p>(<a href="#contents">Back to Contents</a>)</p>
<p><b>DLTrainer</b> is a generalized Python framework for deploying neural networks. It currently supports two services: one for reinforcement learning (RL), and one for supervised learning (SL). Note that DLTrainer is a <em>deployment</em> environment &ndash; it is not a platform for <em>developing</em> neural networks, and as such is not dependent on any particular development platform, such as pytorch or tensorflow. You can develop using your favorite platform, and deploy it within DLTrainer, as long as your Python environment is set up to support your platform.</p>
<p><b>The RL service</b> provides a simple framework for deploying Python reinforcement-learning networks against training data from (possibly remote) DLEnvironment sims. Neural-network implementations can be written as stand-alone Python modules; the trainer automatically handles communication with DLEnvironment sims, as well as batching and routing of DLEnvironment data to the user-supplied network implementations.</p>
<p>DLTrainer supports communication with DLEnvironment via the DLBroker router. The broker allows multiple DLEnvironment sims to be routed to multiple copies of DLTrainer, as shown schematically below.</p>
<p><b>The SL service</b> provides a simple framework for configurable and reproducible deployment of supervised-learning networks. The service allows parallel deployment of multiple independent SL experiments, or multiple copies of the same SL network with different configurations, or both, and captures the scripts, configuration and output of the training sessions. Just as with the RL service, the SL service of the trainer is DL-platform-agnostic (i.e., pytorch, tensorflow, etc); SL network implementations must only support configuration via command-line arguments, or provide a lodable training module run method that supports passing a configuration dictionary. The SL service is entirely configurable via a JSON hierarchy, described in more detail below.</p>
<p>NB: As with any Python environment, modules used by your network implementations will have to be discoverable at runtime. You can add the relevant directories to your PYTHONPATH variable, or via the DLTrainer JSON configuration files (see below). For the RL service, which deploys networks as threads of the main applications, these paths must be available to the DLTrainer runtime environment. For the SL service, which spawns networks as python subprocesses, they can be configured via your environment, or via per-module configurable additions specified in the JSON configuration.</p>
<hr/>
<p> <a class="anchor" id="context"></a> </p><h1>Context</h1>
<p>(<a href="#contents">Back to Contents</a>)</p>
<p><a class="anchor" id="rl_context"></a> </p><h2>The RL Problem</h2>
<p>The general problem of simulations using reinforcement learning (RL) requires that at each step of the simulation, state data for AI instances can be pushed to a neural network, and that the network return appropriate actions to the AI instances.</p>
<p>Traditionally, this requires strong coupling between the simulation generating state data and the RL networks consuming it. This is less than ideal in a research context, because of platform mismatch between data generation (Windows, for gaming sims for example) and neural-network processing (often unix-based), and also because it makes on-the-fly reconfiguration of sims, or rapid iteration in developing network implementations, difficult at best.</p>
<p><br />
 The DLToolkit includes several components to address these issues: <br />
 <br />
</p>
<p><b>DLEnvironment</b> The DLEnvironment library provides a simple layer allowing for decoupling of data generation and processing, by providing a generic remote interface as a C++ DLL that can be integrated with a simulation to provide the capability to configure AI instances on the fly, and that allows the sim to communicate state data and receive actions from a remote training service. The remote framework uses simple JSON configuration files to specify the trainer IP address and port, and the trainer provides the AI configuration when the sim connects.</p>
<p><b>DLTrainer</b> The DLTrainer codebase provides a general-purpose Python framework for receiving and processing data from multiple sim environments, and batching and routing the data between multiple network implementations. The framework allows network implementations to be written as standalone modules and deployed within the framework, provided they adhere to a simple API. With the remote framework provided by the DLEnvironment library, the trainer can be stopped and restarted and any running sims will automatically reconnect.</p>
<p><b>DLBroker</b> The combination of DLEnvironment and DLTrainer provides a generic <em>many-to-one</em> architecture, allowing many sim environments to send data simultaneously to a single trainer (possibly running multiple network implementations). DLBroker extends this architecture to support <em>many-to-many</em> connections. Trainers and sim environments connect to the broker instead of each other, and the broker handles auto-routing between data generators (sim environments) and data consumers (trainers).</p>
<ul>
<li>This allows many instances of a sim environment to be run without needing separate configuration files for each instance (all environments connect to the same broker, instead of to separate trainers)</li>
<li>Allows the broker to automatically handle routing of data between simulation environments and the trainers that can consume them.</li>
<li>Allows the broker to balance data distribution among connected trainers.</li>
<li>Allows sim data to be forwarded to multiple trainers (i.e., for supervised learning applications, or separate control of AI instances by different servers)</li>
</ul>
<p><br />
 In the context of the DLToolkit components, DLTrainer's role is indicated schematically below:</p>
<div class="image">
<img src="images/DLToolkitArch.png"  align="center" alt="DLToolkit Architecture" width="50%"/>
</div>
<p><a class="anchor" id="sl_context"></a> </p><h2>The SL Problem</h2>
<p>Ease of deployment and repeatability are common concerns among the research groups we've talked to. Many SL training modules are driven by command-line configuration which is not easily retrievable or reproducible once the run is finished, variation of configurations generally requires significant manual intervention, and deployments are not easily automated or scalable.</p>
<p>The DLTrainer SL service attempts to improve this situation by providing a simple-to-use JSON front-end for configuration of all SL modules, with a parameter hierarchy that makes it easy to deploy multiple copies of a network with variations of a base configuration.</p>
<p>The service also attempts to mitigate the difficulty of managing training sessions by capturing output and errors in a self-contained, timestamped directory structure that mirrors the configuration of the SL service (see below). Each training session deployed in DLTrainer creates new entries that preserve the results of previous runs, and include the configuration and self-contained scripts which can be used to exactly reproduce the session, if desired.</p>
<p>The service includes a configurable endtime for each deployed network, so that no manual intervention is required to perform, for example, a battery of fixed-length tests.</p>
<p>The service also includes a built in scheduler, so that a large set of fixed-length network variations can be configured at once and deployed on a single node, and the trainer will start each queued network as resources become available, including automatic detection of GPU resource availability.</p>
<p>Next steps may include</p>
<ul>
<li>metrics monitoring and metrics-based session management (abort, restart, iteration)</li>
<li>remote configuration and control of SL training sessions deployed in DLTrainer via the DLBroker</li>
<li>dataset management and automatic routing of remote datasets to SL training sessions via the DLBroker</li>
</ul>
<hr/>
<p> <a class="anchor" id="config"></a> </p><h1>Trainer Configuration</h1>
<p>(<a href="#contents">Back to Contents</a>)</p>
<p><a class="anchor" id="config_overview"></a> </p><h2>Overview</h2>
<p>The trainer ultimately requires a single JSON file to configure the RL and SL services. When invoked with no arguments, DLTrainer loads the trainer config file specified by the default <b>TrainerConfig</b> tag (see <a href="#config_defaults">Parameter Defaults</a>).</p>
<p>You can also specify the configuration file by using the <code>trainerconfig</code> commandline argument, like:</p>
<p>```shell python DLTrainer.py trainerconfig="path/to/my/config_file.json" ```</p>
<p>The philosophy behind configuring DLTrainer is that you should have to write as little JSON as possible to use it. The trainer therefore keeps its own set of default values for all required parameters; you can optionally override any of these parameters in your trainer config file, or omit them and the trainer will use the default values.</p>
<ul>
<li>Notes on parameter defaults are given in <a href="#config_defaults">Parameter Defaults</a>.</li>
<li>The general structure of the trainer config file is discussed in <a href="#config_trainer">Trainer Configuration File</a>.</li>
<li>Details about configuring the SL service are given in <a href="#config_sl_service">Using the SL Service</a>.</li>
<li>Details about configuring the RL service are given in <a href="#config_rl_service">Using the RL Service</a>.</li>
</ul>
<p><a class="anchor" id="config_semantics"></a> </p><h2>Special JSON-Parsing Semantics</h2>
<p>The DLTrainer JSON parser provides some non-standard functionality:</p>
<ul>
<li><p class="startli"><b>Recursive Substitution</b></p>
<p class="startli">Any JSON tag can point to a block of valid JSON, or equivalently to a JSON file containing that block. If the trainer detects a '.json' file extension, it will (recursively) insert the corresponding JSON into the final configuration.</p>
</li>
<li><p class="startli"><b>Path Expansion</b></p>
<p class="startli">Any JSON tag that is detectably a file or path name will be expanded. Detectable paths include filenames with ".json" extensions, or any tag including the name "path" (e.g., "OutputPath", "AddPath"). Either absolute or relative paths (i.e., "../path/to/some/file") are acceptable, and environment variable substitution is supported on all platforms. Relative paths are always expanded relative to the directory of the JSON file containing the tag.</p>
</li>
<li><p class="startli"><b>Comments</b></p>
<p class="startli">The DLTrainer JSON parser treats any line preceded with a '#' character as a comment. These lines are stripped out before parsing.</p>
</li>
<li><p class="startli"><b>AddPath Directive</b></p>
<p class="startli">The tag <b>AddPath</b>, pointing to a path or a list of paths, has several meanings, depending on where it occurs:</p>
<p class="startli">At the top-level in the trainer config file, it causes the trainer to add the specified paths to its system path (equivalent to adding them to your PYTHONPATH environment variable), which will affect all subsequent resolution of python modules referenced in the configuration.</p>
<p class="startli">In an SL module configuration, it causes those paths to be added to the system path only for the spawned subprocesses corresponding to that SL module. Thus, modules can reference the same python module name in different contexts, without namespace conflicts (for example, many experiments start their networks with modules called 'training.py'. This allows each module to specify <em>which</em> training.py it means).</p>
</li>
<li><p class="startli"><b>Include Directive</b></p>
<p class="startli">The tag <b>Include</b> is the equivalent of direct JSON substitution when encountered by the trainer. I.e., if an <code>"Include"</code> tag points to a JSON file, it is equivalent to inserting the contents of that file at the current JSON block level. This allows options used by multiple JSON files to be placed in a separate file and simply included where desired.</p>
</li>
</ul>
<p><a class="anchor" id="config_defaults"></a> </p><h2>Parameter Defaults</h2>
<p>Default values for all services are <em>always</em> taken from the file <code>config/DLTrainer/Parameter_Defaults.json</code>. You can edit this file directly if you wish to change default behaviors, or you can simply override them in your trainer config file. To restore this file to program defaults, simply remove it, and it will be re-created the next time you run DLTrainer.</p>
<p>If the file cannot be created, the trainer will simply initialize parameters internally, but you will not be able to modify the default behaviors globally.</p>
<p>The structure of this file mirrors the service hierarchy of DLTrainer itself, at the highest level containing default parameters used to configure the DLTrainer process itself, and sub-blocks labeled "RL" and "SL", containing default parameters for the RL and SL services, respectively.. Defaults for the services can be overridden for any module included in the service, described in more detail below.</p>
<p>For example, Parameter_Defaults.json might look something like this:</p>
<p>```javascript { #---------------------------------------------------------&mdash; </p><h1>Parameters for DLTrainer</h1>
<p>#---------------------------------------------------------&mdash;</p>
<h1>True to print trainer config JSON on startup</h1>
<p>"ConfigPrint" : false,</p>
<h1>Name of the trainer configuration file</h1>
<p>"TrainerConfig" : "Trainer_Config.json",</p>
<p>...,</p>
<p>"RL" : { #---------------------------------------------------------&mdash; </p><h1>Parameters for RL</h1>
<p>#---------------------------------------------------------&mdash;</p>
<h1>Broker IP Address : port</h1>
<p>"BrokerAddress" : "127.0.0.1:9505",</p>
<h1>Local HTTP Server port</h1>
<p>"HTTPServerPort" : 8505,</p>
<p>...</p>
<p>},</p>
<p>"SL" : { #---------------------------------------------------------&mdash; </p><h1>Parameters for SL</h1>
<p>#---------------------------------------------------------&mdash;</p>
<h1>Name of the directory under which to capture SL output (no path will default to system tempdir)</h1>
<p>"OutputPath" : null,</p>
<h1>True if SL networks require GPUs (used to determine if the trainer should wait for resource availability)</h1>
<p>"RequiresGpu" : false,</p>
<p>...</p>
<p>}</p>
<p>} ```</p>
<p><a class="anchor" id="config_trainer"></a> </p><h2>Trainer Configuration File</h2>
<p>Your trainer configuration file should have the same structure as Parameter_Defaults.json. On startup, your configuration file is merged with Parameter_Defaults.json to produce the trainer's final configuration. Parameters at the highest level override defaults for the trainer process itself, and parameters in the "RL" and "SL" blocks override default parameters for those services.</p>
<p>For example, if your trainer config file contained the blocks:</p>
<p>```javascript { "ConfigPrint" : true,</p>
<p>"RL" : { "BrokerAddress" : "192.168.0.1:9606", "Modules" : ["../simpleTrainingApp/Trainer_Config.json", "../aleApp/Dqn_Config.json"] },</p>
<p>"SL" : { "OutputPath" : "$DEFAULT_SL_OUTPUT_PATH", "Modules" : ["../DLTest/Trainer_Config.json", "../DLAA/Reference/Trainer_Config.json"] }</p>
<p>} ```</p>
<p>then the value of "ConfigPrint" would override the trainer's value from Parameter_Defaults.json above, broker address <code>192.168.0.1:9606</code> would be used instead of <code>127.0.0.1:9505</code> for the RL service, and environment variable <code>$DEFAULT_SL_OUTPUT_PATH</code> would be used as the default output path for all SL modules.</p>
<p>Each service block in your trainer config file is expected to contain a list of modules (i.e., experiment type) that that service will deploy. These can be lists of valid JSON blocks, or lists of files containing JSON for those modules. Each entry defines the complete configuration for a specific network type that the service will deploy. Each entry in the list inherits the parameter values of the enclosing block (so for example, each SL module above would inherit <code>"OutputPath" : "$DEFAULT_SL_OUTPUT_PATH"</code>), and any inherited parameter can be overridden in the configuration for each module.</p>
<p>The module configuration is described for each service below, in <a href="#config_sl_service">Using the SL Service</a>, and <a href="#config_rl_service">Using the RL Service</a>, respectively.</p>
<p>Omitting the block for any service will simply cause that service not to be started.</p>
<p><em>NB: any configuration parameter can also be overridden on the commandline using the same namespace hierarchy (case-insensitive). For example:</em></p>
<p>```shell python DLTrainer.py configprint=true rl.brokeraddress="192.168.0.1:9606" sl.outputpath="$DEFAULT_SL_OUTPUT_PATH" ```</p>
<p><em>would also set these parameters.</em></p>
<hr/>
<p> <a class="anchor" id="config_sl_service"></a> </p><h1>Using the SL Service</h1>
<p>(<a href="#contents">Back to Contents</a>)</p>
<p><a class="anchor" id="config_sl_json"></a> </p><h2>SL Service JSON Configuration</h2>
<p>As described in <a href="#config_trainer">Trainer Configuration File</a>, the SL service configuration includes a <code>"Modules"</code> tag, consisting of a list of modules (experiments) that the service wil spawn. Each entry in the <code>"Modules"</code> list should be a JSON block (or a file containing a JSON block) that includes <em>at least</em> the following tags (for example, the following might be the contents of <code>../DLAA/Reference/Trainer_Config.json</code>, referenced above):</p>
<p>```javascript { </p><h1>These are all DLTrainer-specific arguments</h1>
<p>"Name" : "DLAA", "Impl" : "imgrst_recurrent_DLTrainer"</p>
<h1>Anything in the Config block is specific to the implementation</h1>
<h1>(i.e., passed to the implementation, and not used by the trainer)</h1>
<p>"Config" : { "gpu" : 0 } } ```</p>
<ul>
<li><p class="startli">The <b>Name</b> tag is used to identify the module for logging, constructing output paths, etc.</p>
<p class="startli">The <b>Name</b> tag is also used to provide support for command-line manipulation of parameters for your module. For example:</p>
<p class="startli">```shell python DLTrainer.py sl.dlaa.impl="someOtherModule" ```</p>
<p class="startli">would override the value of the "Impl" tag in the configuration above, while</p>
<p class="startli">```shell python DLTrainer.py sl.dlaa.config.gpu=2 ```</p>
<p class="startli">would override the value of the "gpu" tag inside the "Config" block.</p>
</li>
<li>The <b>Impl</b> tag should point to your custom top-level python module used to start networks of this type. The configuration block above says that your network should be started by calling the 'run' method of module imgrst_recurrent_DLTrainer.py. (see <a href="#config_sl_python">SL Network Implementations</a> for a description of what DLTrainer expects from your network implementation).</li>
<li>If present, the <b>Config</b> block contains any network implementation-specific configuration. These values will be passed by the trainer to your implementation as command-line arguments, or a config dictionary, depending on how your process is configured (see below)</li>
<li>Additionally, if the <b>ExeType</b> tag is present, it allows you to control how your module is launched by the trainer. The following value are supported:<ul>
<li>"python_cmdline" &ndash; this is the default. Your implemention is treated as a python main, and the trainer passes the config parameters as a set of command-line switches to that process</li>
<li>"python_loadable" &ndash; Your implementation is treated as a loadable python module exporting a <code>run(config)</code> method (see below). The trainer will load it and passthe configuration in as a python dictionary.</li>
<li><p class="startli">"other" &ndash; Not a python script, but an arbitrary binary. The trainer calls the executable and passes the config parameters as a set of command-line switches to that process. See <a href="#config_sl_python">SL Network Implementations</a> below.</p>
<p class="startli">For the commend-line versions, a set of additional JSON keys allows you to control how your config is turned into command-line argunents.</p>
</li>
</ul>
</li>
<li>In addition to these tags, DLTrainer supports a growing list of parameters for configuring output, deployment control, etc., of your module's networks. See the file <code>Parameter_Defaults.json</code> for a full list and inline documentation.</li>
</ul>
<p>These parameters are used by the trainer when spawning your network process. Any <em>other</em> parameters you specify in the block for your module will be passed to your module as a python dictionary on startup (see <a href="#config_sl_python">SL Service Network Implementations</a> below).</p>
<p>As shown, this example configuration would cause a single copy of a DLAA network to be run by the SL service. However, if your configuration contains the <b>Networks</b> tag, a network for each entry in that list will be run. For example:</p>
<p>```javascript { </p><h1>These are all DLTrainer-specific arguments</h1>
<p>"Name" : "DLAA", "Impl" : "imgrst_recurrent_DLTrainer",</p>
<h1>These are user-defined (in this case DLAA-specific) arguments</h1>
<p>"Config" : { "Include" : "Cmdline_Defaults.json", "DLAAConfigs" : "Configs.json", "ActiveConfig" : "msalvi-dgx-wrae" },</p>
<h1>Networks to run. These inherit all parameters above, and allow</h1>
<h1>you to override selected parameter values: trainer service</h1>
<h1>parameters are accessed directly, and "Config" keywords above</h1>
<h1>can be modified by reopening that namespace</h1>
<p>"Networks" : [{"Config" : {"gpu" : 0, "CROP_SIZE" : 128, "BATCH_SIZE" : 2}}, {"Config" : {"gpu" : 1, "CROP_SIZE" : 128, "BATCH_SIZE" : 4}}, {"Config" : {"gpu" : 2, "CROP_SIZE" : 128, "BATCH_SIZE" : 8}}, {"Config" : {"gpu" : 3, "CROP_SIZE" : 192, "BATCH_SIZE" : 2}}] } ```</p>
<p>would cause the SL service to run four networks simultaneously. Just like the "Modules" list in the trainer's config, each network in the "Networks" list inherits all parameters from its parent block, so you can just override parameters of interest. Trainer SL Service parameters are specified directly, and implementation-specific parameters are specified by reopening the "Config" namespace. In the example above, each network inherits everything in the parent "Config" block, and only varies "gpu", "CROP_SIZE", and "BATCH_SIZE".</p>
<p><em>NB: And just as each module's parameters can be overridden on the commandline, you can override any network's parameters too. For example:</em></p>
<p>``` python DLTrainer.py sl.dlaa.network_0.config.crop_size=192 ```</p>
<p><em>would override the network specification above.</em></p>
<p>Two additional parameters control how the python network implementation is run:</p>
<ul>
<li><p class="startli">"AddPath"</p>
<p class="startli">This can be a path, or a list of paths.</p>
<p class="startli">If using "ExeType" : "python_cmdline" or "python_loadble", the trainer will add these to the python path (equivalent to the PYTHONPATH) for the spawned subprocess. This allows you to invoke implementations outside of the trainer's system path, and to re-use the same implementation name (e.g., the ubiquitous 'training.py') for different modules.</p>
<p class="startli">If using "ExeType" : "other", the trainer will add these to the system path for the spawned process (equivalent to the PATH variable).</p>
</li>
<li><p class="startli">"PyPath"</p>
<p class="startli">If specified, the directory in which to execute your network. Some training implementations assume that the code is being executed in a standard place (i.e., to look for data files, etc). This is not in general the case when run from the trainer. But if you specify "PyPath", the trainer script will explicitly chdir to that directory before running your implementation.</p>
<p class="startli">NB: This is not in general the same as auto-detecting the path in which your top-level implementation resides, which is why I leave it as a user-definable parameter.</p>
</li>
</ul>
<p><a class="anchor" id="config_sl_python"></a> </p><h2>SL Service Network Implementations</h2>
<p><b>Command-line Python Modules</b></p>
<p>(<code>"ExeType" : "python_cmdline"</code>)</p>
<p>In this case, the trainer assumes your implementation specified in the "Impl" tag is a python command line (invoked like <code>python training.py --boolswitch --param1 val1 --param2 val2</code>). The trainer will take parameters from your network JSON "Config" block and call your implementation with the equivalent command line. Construction of the command line is controlled by the "CmdlinePrefix" and "CmdlineSep" keywords.</p>
<p><b>Loadable Python Modules</b></p>
<p>(<code>"ExeType" : "python_loadable"</code>)</p>
<p>In this case or use with DLTrainer, your module (the python code pointed to in the "Impl" tag for your module) must be a loadable python module (i.e., not a python 'main') that exports a run method with the signature:</p>
<p>```python def run(config): </p><h1>Start a network, using parameters passed in the config dictionary</h1>
<p>... ```</p>
<p>DLTrainer wraps this method in its own script to deploy networks, and passes in a dictionary of custom parameter values specified in the JSON above.</p>
<p>For example:</p>
<p>```javascript { </p><h1>These are all DLTrainer-specific arguments</h1>
<p>"Name" : "DLAA", "Impl" : "imgrst_recurrent_DLTrainer",</p>
<h1>These are user-defined (in this case DLAA-specific) arguments</h1>
<p>"Config" : { "Include" : "Cmdline_Defaults.json", "DLAAConfigs" : "Configs.json", "ActiveConfig" : "msalvi-dgx-wrae" } } ```</p>
<p>would cause a python dictionary containing keys <b>DLAAConfigs</b>, <b>ActiveConfig</b>, and any key defined in <code>Cmdline_Defaults.json</code> to the run method of <code>imgrst_recurrent_DLTrainer.py</code> when starting DLAA networks (see <a href="#config_semantics">Special Semantics</a> for a discussion of the <b>Include</b> tag).</p>
<p><b>Other Executable</b></p>
<p>(<code>"ExeType" : "other"</code>)</p>
<p>In this case, the trainer assumes that the implementation pointed to by the "Impl" keyword is an arbitrary executable. The trainer will take parameters from your network JSON "Config" block and call your implementation with the equivalent command line. Construction of the command line is controlled by the "CmdlinePrefix" and "CmdlineSep" keywords.</p>
<p><a class="anchor" id="config_sl_output"></a> </p><h2>SL Network Output</h2>
<p>As networks are spawned, the service captures network artifacts in a standard place. This is controlled by the "OutputPath" parameter (which can be separately controlled at the service or module level). Each time the trainer is run, it creates a time-stamped directory under this path, subdirectories for all configured modules beneath that time-stamped directory, and subdirectories for each network beneath those module dirs.</p>
<p>Inside each network directory, the trainer places a python script used to start the network (and that can be used to exactly reproduce the network as run by the trainer), and the captured stdout and stderr of the spawned network process.</p>
<p>The trainer also creates an output directory inside each network directory where implementations can place output artifacts (e.g., network weights, convergence metrics, etc). If your config specifies a "ModOutputTag", pointing to a config keywork used by your implementation to set an output directory, the trainer will pass this output directory to your implementation .</p>
<p>As an example, suppose your configuration was:</p>
<p>```javascript { </p><h1>These are all DLTrainer-specific arguments</h1>
<p>"Name" : "DLAA_hpg18-submission", "Impl" : "imgrst_recurrent_DLTrainer", "OutputPath" : "/raid/data/DLAA/Training/Output", "ModOutputTag" : "folder_path",</p>
<h1>These are user-defined (in this case DLAA-specific) arguments</h1>
<p>"Config" : { "Include" : "Cmdline_Defaults.json", "DLAAConfigs" : "Configs.json", "ActiveConfig" : "msalvi-dgx-wrae", "folder_path" : "/path/to/some/dir" },</p>
<h1>Networks to run. These inherit all default parameters above, and override selected parameter values</h1>
<p>"Networks" : [{"Config" : {"gpu" : 0, "CROP_SIZE" : 128, "BATCH_SIZE" : 2}}, {"Config" : {"gpu" : 1, "CROP_SIZE" : 128, "BATCH_SIZE" : 4}}, {"Config" : {"gpu" : 2, "CROP_SIZE" : 128, "BATCH_SIZE" : 8}}, {"Config" : {"gpu" : 3, "CROP_SIZE" : 192, "BATCH_SIZE" : 2}}] } ```</p>
<p>Then DLTrainer would create a directory structure that looks like:</p>
<p>```shell ngvpn01-173-211.dyn.scz.us.nvidia.com:~:&gt;cd /raid/data/DLAA/Training/Output/ ngvpn01-173-211.dyn.scz.us.nvidia.com:Output:&gt;tree -L 4 |&ndash; 2018-05-01T15_50_05.446925 |&ndash; DLAA_hpg18-submission |&ndash; Network_0 | |&ndash; output | |&ndash; script.py | |&ndash; stderr.txt | |&ndash; stdout.txt |&ndash; Network_1 | |&ndash; output | |&ndash; script.py | |&ndash; stderr.txt | |&ndash; stdout.txt |&ndash; Network_2 | |&ndash; output | |&ndash; script.py | |&ndash; stderr.txt | |&ndash; stdout.txt |&ndash; Network_3 |&ndash; output |&ndash; script.py |&ndash; stderr.txt |&ndash; stdout.txt ```</p>
<p>(here, <code>script.py</code> is the python wrapper around the user-supplied network implementation (including the complete configuration of the network), <code>stdout.txt</code> and <code>stderr.txt</code> are the captured stdout/stderr of the network process, and output is the directory DLTrainer creates for the network's output artifacts.)</p>
<p>Because the config above specified <code>"ModOutputTag" : "folder_path"</code>, the trainer would pass its output directories above to each copy of the network implementation, as the <code>folder_path</code> argument.</p>
<p><a class="anchor" id="sl_coordination"></a> </p><h2>Coordinating SL Networks</h2>
<ul>
<li><p class="startli"><b>Timed Runs</b></p>
<p class="startli">A network can be run for a fixed length of time, by specifying the "TimeoutSec" parameter (which you can specify separately at the service-level, module-level or network-level). The SL service will stop the network when the specified number of seconds has elapsed.</p>
</li>
<li><p class="startli"><b>Delayed Runs</b></p>
<p class="startli">You can insert a configurable delay after starting a network, by using the "DelaySec" parameter. This can be useful when spawning multiple networks on a multi-GPU machine, where the network implementation relies on GPU utilization to implicitly select a GPU.</p>
</li>
<li><p class="startli"><b>Scheduling Runs</b></p>
<p class="startli">The SL service provides a simple scheduling mechanism, for configuring many timed networks on a single node, where the number of desired networks may exceed the GPU resources of that node.</p>
<p class="startli">Parameters controlling the scheduler include:</p><ul>
<li><p class="startli">"RequiresGpu"</p>
<p class="startli">This should be set to <code>true</code> if a network requires GPU resources</p>
</li>
<li><p class="startli">"GpuTag"</p>
<p class="startli">This parameter tells the trainer which module-specific JSON tag specified the GPUs a network wants to use. This is required because there is no standard for specifying GPUs, some experiments using "gpu", others using "gpus".</p>
<p class="startli">The trainer does however expect either valid GPU indices, or the value <code>-1</code> to indicate that any GPU is ok. For example:</p>
<p class="startli">``` "gpu" : [0, 1] "gpu" : -1 "gpu" : [0, 1, 2, -1] ```</p>
</li>
<li><p class="startli">"MaxGpuNetworks"</p>
<p class="startli">This parameter specifies the maximum number of GPU networks that can be run simultaneously. The trainer will take the smaller of this or the number of GPUs actually present, to determine the max allowable number of networks.</p>
</li>
<li><p class="startli">"MaxNetworks"</p>
<p class="startli">This parameter specifies the maximum number of any network (GPU or CPU-based) that can be run simultaneously.</p>
</li>
</ul>
<p class="startli">When the service starts, the scheduler will spawn whatever configured networks it can, up to the network or resource limits, queuing the remainder to be run when resources are available. As timed runs complete, the scheduler will iterate through the queue of waiting networks, matching GPU requirements with available resources.</p>
</li>
</ul>
<hr/>
<p> <a class="anchor" id="config_rl_service"></a> </p><h1>Using the RL Service</h1>
<p>(<a href="#contents">Back to Contents</a>)</p>
<p>Because DLTrainer is a training service that can support multiple neural networks processing data from different DLEnvironment simulation types, this file should specify any parameters required by the trainer itself (see <a href="#trainer_config">DLTrainer Configuration Files</a> below), as well as the JSON needed to configure any sims that will talk to the trainer (see <a href="#env_config">DLEnvironment Configuration Files</a> below).</p>
<p><a class="anchor" id="config_rl_dltrainer"></a> </p><h2>DLTrainer Configuration Files</h2>
<p>As noted above, the <code>trainerconfig</code> commandline argument (or the default <code>TrainerConfig</code> parameter) specifies a configuration file for DLTrainer itself. This is used to configure how DLTrainer communicates with the broker, what sims the trainer supports, and any behavioral parameters for the trainer itself.</p>
<p>For example, suppose the default config/DLTrainer/Trainer_Config.json file contains:</p>
<p>``` { "RL" : { "Modules" : ["../simpleTrainingApp/Trainer_Config.json", "../aleApp/Trainer_Config.json"] } } ```</p>
<p>This says that the trainer supports data from two different types of DLEnvironment sims, whose configurations are given in <code>config/simpleTrainingApp/Trainer_Config.json</code> and <code>config/aleApp/Trainer_Config.json</code>, respectively.</p>
<p>Absolute paths can be used, and support any exported environment variables. Any time a relative path to a json file is encountered while parsing another json file, the path is taken to be relative to the directory of the immediate parent configuration file.</p>
<p>As noted above, the default value for any valid trainer parameter can be overridden by specifying it in the trainer config file, so:</p>
<p>``` { "RL" : { "BrokerAddress" : "192.168.0.1:9505", "Modules" : ["../simpleTrainingApp/Trainer_Config.json", "../aleApp/Trainer_Config.json"] } } ```</p>
<p>would cause the trainer to contact the broker at IP address 192.168.0.1 instead of the default value.</p>
<p>The <code>"Modules"</code> keyword is required, and specifies a list of simulation types that can be processed by the trainer. Note that this keyword can be a list of JSON files containing the sim configurations (as above), or can instead contain the <em>contents</em> of those files, so the following is also a valid config file:</p>
<p>``` { "RL" : { "Modules" : [ { "Name" : "SimpleTrainingApp", "Config" : "../simpleTrainingApp/Training_Service.json", "Batchsize" : 10,</p>
<p>"Networks" : { "default" : ["staImpl1", "staImpl2"], "special" : "staTestImpl" } },</p>
<p>{ "Name" : "AleApp", "Config" : "../aleApp/Pong_Training_Service.json", "Batchsize" : 2,</p>
<p>"Networks" : { "default" : ["aleImpl1", "aleImpl2"], "special" : "aleTestImpl" } } ] } } ```</p>
<p>For each simulation type handled by the trainer, you must specify a name for the simulation type, via the <code>Name</code> parameter. This tag should match the name published by the corresponding DLEnvironment sims, and is used by the broker to route data between sims and trainers.</p>
<p>For each simulation type, you must also specify a configuration file, via the <code>Config</code> parameter. This configuration will be sent to the DLEnvironment sim by the trainer whenever it initiates a new connection.</p>
<p>Defaults for any other simulation-type-specific parameters in config/DLTrainer/Parameter_Defaults.json can be overridden by specifying them within the JSON block for that sim type. In the example above, <code>Batchsize</code> is 10 for sims of type SimpleTrainingApp, but 2 for typ AleApp.</p>
<p>If a <code>Networks</code> block occurs in the JSON for a simulation type, it controls how networks are deployed for that DLEnvironment type (more on that below at <a href="#impls">Network Implementations</a>). If no networks are defined, the default network implementation is taken from the <code>DefaultImpl</code> parameter.</p>
<p>For details on interpreting DLEnvironment configuration files, see below.</p>
<p><a class="anchor" id="config_rl_dlenv"></a> </p><h2>DLEnvironment Configuration Files</h2>
<p>(<a href="#contents">Back to Contents</a>)</p>
<p>As detailed above, each DLEnvironment block in a trainer config file must specify a configuration file that will be sent to the sim whever it initiates a connection to the trainer, via the <code>Config</code> keyword.</p>
<p>For example, the Trainer_Config.json file in the example above specifies that the contents of <code>config/simpleTrainingApp/Training_Service.json</code> should be sent to simulations of type SimpleTrainingApp when they connect. That file might look like:</p>
<p>``` { "StateOutput": "StateOutput", "StatisticsOutput": "Statistics_Service",</p>
<p>"AIInstances": [ "Training_AIInstance.json" ],</p>
<p>"Experiment": "Experiment.json",</p>
<p>"ApplicationSettings": { "AreaSize": 5.0, "Speed": 0.5, "Distance": 3.0 } } ```</p>
<p>This defines the number of AI Instances (in this case one) and points to a configuration file for each instance in the list (in this case Training_AIInstance.json).</p>
<p>When a DLEnvironment sim connects to the trainer (via the broker or the REST interface), the trainer sends its configuration (in the example above, <code>config/simpleTrainingApp/Training_Service.json</code>) down to the sim.</p>
<p>When sent by the trainer, any JSON files referenced within the DLEnvironment config file hierarchy are replaced by their contents.</p>
<p>The config supports running multiple networks, and the json has been extended to allow things like:</p>
<p>``` { "StateOutput": "StateOutput", "StatisticsOutput": "Statistics_Service",</p>
<p>"AIInstances": [ </p><pre class="fragment">    {
       "Config":"Training_AIInstance.json",
       "Network": "default"
    },

    {
        "Config":"Training_AIInstance.json",
        "Network": "special"
     },

   ],
</pre><p>"Experiment": "Simple_Experiment.json",</p>
<p>"ApplicationSettings": { "AreaSize": 5.0, "Speed": 0.5, "Distance": 3.0 } } ```</p>
<p>Note that each element of the AIInstance array has been replaced with JSON which specifies the config for that instance, as well as a network to process data for that instance. These network names are labels that must defined in the trainer config via a block of commands like:</p>
<p>``` "Networks": { "default": "experimentImpl", "special": "specialImpl" }, ```</p>
<p>The example above tells the trainer that two different network implementations may be used (and defines the python modules to use for the implementations), declares two AI instances, and allows the configuration to associate each AI instance with a different network. On loading the configuration file, the trainer will spawn network objects for any network in the list that is actually referenced by an AI configuration.</p>
<p>Any AI instance that doesn't specify a network will be connected to the default network. If a <code>Networks</code> block is defined in the trainer config file, this will be the network labeled <code>default</code>. If no <code>Networks</code> block is present, or the block doesn't define a default, then this will be the network specified by the <code>DefaultImpl</code> parameter (see discussion of default parameter handling starting at <a href="#trainer_defaults">Default Parameter Values</a> above).</p>
<hr/>
<p> <a class="anchor" id="rl_impls"></a> </p><h2>Network Implementations</h2>
<p>(<a href="#contents">Back to Contents</a>)</p>
<p>DLTrainer is a famework for deploying network implementations against DLEnvironment data.</p>
<p>It allows you to:</p>
<ul>
<li>deploy a single default network to process all DLEnvironment data</li>
<li>deploy multiple instances of the same network that DLEnvironment data of the same type will be distributed among</li>
<li>deploy different network implementations that data for the same sim type will be distributed among</li>
<li>deploy different network implementations for different AI instances in the same DLEnvironment sim</li>
<li>deploy different network implementations for different types of DLEnvironment sim data</li>
</ul>
<p>or all of the above. These are explained in more detail below.</p>
<p>The name of the default network implementation module to deploy can be specified via configuration files, as discussed above, or via commandline arguments when starting the trainer. For example:</p>
<p>``` python DLTrainer.py defaultimpl=experimentImpl ```</p>
<p>would load experimentImpl.py as the default network implementation. Network implementations known to DLTrainer are contained under the <code>impls</code> directory, but network implementations can live anywhere in your PYTHONPATH path.</p>
<p>Each network implementation must be self-contained (any module it depends on must be loaded by the implementation), and must export the following functions:</p>
<ul>
<li><p class="startli"><code>getActionArgs()</code></p>
<p class="startli">This should return a python list of arguments. The length of the list determines how many copies of the network implementation the trainer will run, and the ith element of the list is passed to the ith copy of the network every time takeAction(arg, state) is called (on a tick from a client - see below)</p>
</li>
<li><p class="startli"><code>takeAction(args, state)</code></p>
<p class="startli">This is called on every tick from a client, with the initialization arguments (if any) for this network, and the current state of the client sim. It should return the actions to be sent back to the client from the training network.</p>
</li>
</ul>
<p>Additionally, you can control what constitutes an episode and experiment by defining the optional functions:</p>
<ul>
<li><p class="startli"><code>checkEpisodeComplete(args, state, numTicks, ticksPerEpisode)</code></p>
<p class="startli">This is called on every tick to determine if the current episode has finished. The function is passed the initialization arguments (if any) for this network, the current state, the number of ticks in the current episode, and the configured number of ticks per episode (from the <code>TicksPerEpisode</code> parameter for this simulation type).</p>
<p class="startli">If your implementation doesn't define this function, an episode is considered complete when numTicks == ticksPerEpisode.</p>
</li>
<li><p class="startli"><code>checkExperimentComplete(args, state, numEpisodes, episodesPerExperiment)</code></p>
<p class="startli">This is called whenever an episode finishes to determine if the current experiment has finished. The function is passed the initialization arguments (if any) for this network, the current state, the number of episodes in the current experiment, and the configured number of episodes per experiment (from the <code>EpisodesPerExperiment</code> parameter for this simulation type).</p>
<p class="startli">If your implementation doesn't define this function, an experiment is considered complete when numEpisodes == episodesPerExperiment.</p>
</li>
</ul>
<p><a class="anchor" id="rl_network_examples"></a> </p><h2>Examples</h2>
<p><a class="anchor" id="rl_network_ex1"></a> <b>Same default network, multiple copies</b></p>
<p>Suppose your trainer config contains the section:</p>
<p>```javascript "Networks": { "default": "implA" }, ```</p>
<p>and your DLEnvironment config looks like:</p>
<p>```javascript "AIInstances": [ "Training_AIInstance.json", "Training_AIInstance.json" ],... ```</p>
<p>and implA.py defines (in pseudo-code: arg1 and arg2 must be mutable Python types):</p>
<p>```python def getActionArgs(): return [arg1, arg2] ```</p>
<p>When the trainer starts, it will instantiate two copies of implA, one which calls <code>implA.takeAction(arg1, state)</code>, and one which calls <code>implA.takeAction(arg2, state)</code>, round-robining any client that references the <code>default</code> network between them, as in the following diagram:`</p>
<div class="image">
<img src="images/multCopy.png"  alt="Multiple Copies" width="50%"/>
</div>
<p><a class="anchor" id="rl_network_ex2"></a> <b>Multiple default networks, single copy of each</b></p>
<p>Suppose instead that your trainer config contains the section:</p>
<p>```javascript "Networks": { "default": ["implA", "implB"] }, ```</p>
<p>and each impl defines:</p>
<p>```python def getActionArgs(): return [arg1] ```</p>
<p>When the trainer starts, it will instantiate one copy of implA and one copy of implB, each of which calls <code>takeAction(arg1, state)</code>, round-robining any client that references the <code>default</code> network between the two networks, as in the following diagram:</p>
<div class="image">
<img src="images/multImpl.png"  alt="Multiple Copies" width="50%"/>
</div>
<p><a class="anchor" id="rl_network_ex3"></a> <b>Multiple default networks, multiple copies</b></p>
<p>If your trainer config contains the section:</p>
<p>```javascript "Networks": { "default": ["implA", "implB"] }, ```</p>
<p>and each impl defines:</p>
<p>```python def getActionArgs(): return [arg1, arg2] ```</p>
<p>when the trainer starts, it will instantiate two copies of <em>both</em> implA and implB, one copy calling <code>takeAction(arg1, state)</code>, and one copy calling <code>takeAction(arg2, state)</code>, round-robining any client that references the <code>default</code> network between the four networks, as in the following diagram:`</p>
<div class="image">
<img src="images/multImplCopy.png"  alt="Multiple Copies" width="50%"/>
</div>
<p><a class="anchor" id="rl_network_ex4"></a> <b>Multiple AI networks</b></p>
<p>If your trainer config contains the section:</p>
<p>```javascript "Networks": { "default": "implA", "test" : "implB" }, ```</p>
<p>and the DLEnvironment config references these networks, like:</p>
<p>```javascript "AIInstances": [ { "Config":"Training_AIInstance.json", "Network": "default" }, { "Config":"Training_AIInstance.json", "Network": "test" }, ], ```</p>
<p>and implA defined:</p>
<p>```python def getActionArgs(): return [argA1, argA2] ```</p>
<p>and implB defined:</p>
<p>```python def getActionArgs(): return [argB1] ```</p>
<p>when the trainer starts, it will instantiate two copies of implA, and one copy of implB. The first AI instance of each client connection will be round-robined between the two copies of implA, and the second AI instance will always be routed to the single copy of implB, as in the following diagram:`</p>
<div class="image">
<img src="images/multImplMultCopy.png"  alt="Multiple Copies" width="50%"/>
</div>
<p><a class="anchor" id="rl_network_ex5"></a> <b>Multiple DLEnv sim types</b></p>
<p>Finally, if your trainer config contains the section</p>
<p>```javascript { "RL" : "Modules" : [ { "Name" : "type1", "Config" : "../simpleTrainingApp/Training_Service.json",</p>
<p>"Networks" : { "default" : ["implA", "implB"], } },</p>
<p>{ "Name" : "type2", "Config" : "../aleApp/Pong_Training_Service.json",</p>
<p>"Networks" : { "default" : "implC" } } ] } } ```</p>
<p>when the trainer starts, it will instantiate three networks: one each of implA, implB and implC (assuming each impl specifies only one copy). Data from sim type1 will be round-robined between implA and implB, and data for sim type2 will be routed to implC, as in the following diagram:</p>
<div class="image">
<img src="images/multCopyMultEnv.png"  alt="Multiple Sim Types" width="50%"/>
</div>
 </div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated on Mon Jul 2 2018 10:26:35 for NVIDIA DeepLearning Dataset Synthesizer (NDDS) by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.8 </li>
  </ul>
</div>
</body>
</html>
